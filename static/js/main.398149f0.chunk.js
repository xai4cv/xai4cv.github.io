(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{104:function(e,a,t){"use strict";t.r(a);var n,l,r=t(0),i=t.n(r),s=t(8),o=t.n(s),c=(t(93),t(21)),m=t(22),u=t(24),p=t(23),d=(t(94),t(143)),h=t(62),E=t(12),g=t(82),b=Object(g.a)({palette:{primary:{light:"#9ccc65",main:"#3b4252",dark:"#33691e",contrastText:"#fff"},secondary:{light:"#ff7961",main:"#f44336",dark:"#ba000d",contrastText:"#000"}},typography:{useNextVariants:!0}}),f=t(34),v=t(139),y=t(140),k=t(138),S=t(146),w=t(144),x=t(145),N=t(141),P=(t(95),t(96),t(74)),I=t.n(P),A=t(136),M=t(7),C=t(137),_={root:{width:"100%",flexGrow:1},buttonLink:{color:b.palette.primary.contrastText,textDecoration:"none"},buttonsSide:{textAlign:"right"},sectionDesktop:(n={display:"none"},Object(f.a)(n,b.breakpoints.up("md"),{display:"flex"}),Object(f.a)(n,"flex",1),Object(f.a)(n,"flexDirection","row-reverse"),n),sectionMobile:(l={display:"flex"},Object(f.a)(l,b.breakpoints.up("md"),{display:"none"}),Object(f.a)(l,"flex",1),Object(f.a)(l,"flexDirection","row-reverse"),l)},z=function(e){Object(u.a)(t,e);var a=Object(p.a)(t);function t(){var e;Object(c.a)(this,t);for(var n=arguments.length,l=new Array(n),r=0;r<n;r++)l[r]=arguments[r];return(e=a.call.apply(a,[this].concat(l))).state={mobileMoreAnchorEl:null,dialogOpen:!1},e.handleMobileMenuOpen=function(a){e.setState({mobileMoreAnchorEl:a.currentTarget})},e.handleMobileMenuClose=function(){e.setState({mobileMoreAnchorEl:null})},e}return Object(m.a)(t,[{key:"render",value:function(){var e=this.props.classes,a=this.state.mobileMoreAnchorEl,t=Boolean(a),n=i.a.createElement(w.a,{anchorEl:a,anchorOrigin:{vertical:"top",horizontal:"right"},transformOrigin:{vertical:"top",horizontal:"right"},open:t,onClose:this.handleMobileMenuClose},i.a.createElement(A.a,{onClickAway:this.handleMobileMenuClose},i.a.createElement(x.a,null,i.a.createElement(S.a,{onClick:this.handleMobileMenuClose},i.a.createElement(C.a,{underline:"none",className:[e.buttonLink,e.buttonsSide].join(" "),href:"mailto:xai4cv2024@googlegroups.com"},i.a.createElement(k.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},i.a.createElement("i",{className:"fa fa-envelope fa-lg"})))))));return i.a.createElement("div",{className:e.root},i.a.createElement(v.a,{position:"static",color:"primary"},i.a.createElement(y.a,null,i.a.createElement(C.a,{underline:"none",align:"left",className:[e.buttonLink,e.grow].join(" "),href:"."},i.a.createElement("b",null,"XAI4CV")),i.a.createElement("div",{className:e.sectionDesktop},i.a.createElement(C.a,{underline:"none",className:[e.buttonLink,e.buttonsSide].join(" "),href:"mailto:xai4cv2024@googlegroups.com"},i.a.createElement(k.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},i.a.createElement("i",{className:"fa fa-envelope fa-lg"})))),i.a.createElement("div",{className:e.sectionMobile},i.a.createElement(N.a,{"aria-haspopup":"true",onClick:this.handleMobileMenuOpen,color:"inherit"},i.a.createElement(I.a,null))),n)))}}]),t}(i.a.Component),H=Object(M.a)(_)(z),T=t(108),V=t(142),R=function(e){Object(u.a)(t,e);var a=Object(p.a)(t);function t(){var e;Object(c.a)(this,t);for(var n=arguments.length,l=new Array(n),r=0;r<n;r++)l[r]=arguments[r];return(e=a.call.apply(a,[this].concat(l))).state={checked:!1},e}return Object(m.a)(t,[{key:"render",value:function(){var e=this.props.classes;return i.a.createElement("div",{className:e.root},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:10,md:9,lg:9,className:e.content},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:12,lg:12},i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Explainable AI for Computer Vision (XAI4CV)"))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Upcoming workshop")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("a",{href:"./workshop_cvpr24"},i.a.createElement("b",null,"The 3rd XAI4CV Workshop at CVPR 2024"))))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Previous workshops")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("a",{href:"./workshop_cvpr23"},i.a.createElement("b",null,"The 2nd XAI4CV Workshop at CVPR 2023"))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("a",{href:"./workshop_cvpr22"},i.a.createElement("b",null,"The 1st XAI4CV Workshop at CVPR 2022"))))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement("img",{className:"img-fluid",width:"100%",alt:"CVPR 2022 Workshop",src:"assets/images/xai4cv_cvpr22.jpeg"})))))))))}}]),t}(i.a.Component),j=Object(M.a)(function(e){return{content:{margin:"0 auto",marginTop:"1.5em"},root:{},gridItem:{padding:e.spacing(1.5)},sectionHeader:{marginTop:"0.15em"},container:{padding:e.spacing(2)},footer:{marginTop:"0.15em",fontSize:14}}})(R),D=t(75),L=t(147),B=Object(M.a)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:e.spacing(2)}}})(function(e){var a=[],t=3;a=e.people?e.people:D.a,e.lgSize&&(t=e.lgSize);var n=a.map(function(a){var n=Math.random();return i.a.createElement(V.a,{item:!0,key:n,xs:12,sm:8,md:6,lg:t},i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(L.a,{className:e.classes.avatar,src:a.img_url})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(T.a,{variant:"subtitle1"},i.a.createElement("b",null,a.name))),i.a.createElement(T.a,{variant:"caption"},a.organization))});return i.a.createElement(V.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start"},n)}),O=t(76),W=Object(M.a)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:e.spacing(2)}}})(function(e){var a=[],t=3;a=e.speakers?e.speakers:O.a,e.lgSize&&(t=e.lgSize);var n=a.map(function(a){var n=Math.random();return i.a.createElement(V.a,{item:!0,key:n,xs:12,sm:8,md:6,lg:t},i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(L.a,{className:e.classes.avatar,src:a.img_url})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(T.a,{variant:"subtitle1"},i.a.createElement("b",null,a.name))),i.a.createElement(T.a,{variant:"caption"},a.organization))});return i.a.createElement(V.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start"},n)}),G=t(4),X=t.n(G),F=t(2),K=t.n(F),U=function(e){Object(u.a)(t,e);var a=Object(p.a)(t);function t(){var e;Object(c.a)(this,t);for(var n=arguments.length,l=new Array(n),r=0;r<n;r++)l[r]=arguments[r];return(e=a.call.apply(a,[this].concat(l))).state={checked:!1},e}return Object(m.a)(t,[{key:"render",value:function(){var e=this.props.classes;return document.title="Workshop",i.a.createElement("div",{className:e.root},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:10,md:9,lg:9,className:e.content},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:12,lg:12},i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,md:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"center"},"The 1st Explainable AI for Computer Vision (XAI4CV) Workshop at CVPR 2022")),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Date:")," Monday, June 20, 2022 ",i.a.createElement("br",null),i.a.createElement("b",null,"Venue:")," New Orleans Ernest N. Morial Convention Center, New Orleans, Louisiana ",i.a.createElement("br",null),i.a.createElement("b",null,"Location:")," 208-210 ",i.a.createElement("br",null),i.a.createElement("b",null,"Motivation:")," Provide a common forum for both computer vision practitioners in the industry and academia to initiate discussions and propose best ways to build explainable models that can benefit the global community.")),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Abstract")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"Computer vision (CV) models are often used to improve applications and products and are very successful. However, these models are usually black-box in nature and do not provide explanations for their predictions, which sometimes leads to confusing behaviour. The current lack of transparency in CV models is one of the biggest barriers in building trust among consumers, often resulting in severe backlash when models make embarrassing mistakes. Trust in the models can be improved by making them fair, easily understandable for everyone, and correctable if necessary. The need for trustworthiness has prompted recent clauses in GDPR regulations that require models to also explain their results in a way a naive consumer can understand.",i.a.createElement("br",null),i.a.createElement("br",null),'With socio-political implications of AI in mind, this workshop aims to motivate proactive adaptation of explainability in computer vision systems. Specifically, our aim is to spark healthy conversations that address building top-performing explainable computer vision systems that not only identify "what" and "where" different visual entities occur in an image, but also provide a human-like reasoning of "why" the model made those predictions. Furthermore, such systems should allow a user to provide feedback and thereby correct potentially harmful mispredictions with minimal effort.')),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Invited Speakers")),i.a.createElement(W,null),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Schedule")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body1",align:"left"},"Video recordings for all sessions available on ",i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtube.com/playlist?list=PLmTZeha8o7E9cRtwmOOuVBzpSsAN0smk1"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," YouTube")),".")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"09:00 AM - 09:10 AM: ")," Opening Remarks - ",i.a.createElement("b",null,"Filip Radenovic")," (Meta AI)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/_FhGrL3Fm2o"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"09:10 AM - 09:50 AM: ")," Spotlight Session 1 (",i.a.createElement("i",null,"8 min + 2 min QA"),")"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,"S 06: ",i.a.createElement("b",null,"HIVE: Evaluating the Human Interpretability of Visual Explanations.")," Sunnie S. Y. Kim, Nicole Meister, Vikram V. Ramaswamy, Ruth C Fong, Olga Russakovsky."),i.a.createElement("li",null,"S 11: ",i.a.createElement("b",null,"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic Filter Attention.")," Yu Yang, Seungbae Kim, Jungseock Joo."),i.a.createElement("li",null,"S 17: ",i.a.createElement("b",null,"Causality for Inherently Explainable Transformers: CAT-XPLAIN.")," Subash Khanal, Benjamin Brodie, Xin Xing, Ai-Ling Lin, Nathan Jacobs."),i.a.createElement("li",null,"S 20: ",i.a.createElement("b",null,"Do learned representations respect causal relationships?")," Lan Wang, Vishnu Boddeti."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/7uysq-qAtr4"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"09:50 AM - 10:30 AM: ")," Coffee Break + Poster Session 1"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p2.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 02")),": ",i.a.createElement("b",null,"Finding and Fixing Spurious Patterns with Explanations.")," Gregory Plump, Marco Tulio Ribeiro, Ameet Talwalkar."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p3.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 03")),": ",i.a.createElement("b",null,"Sanity Simulations for Saliency Methods.")," Joon Sik Kim, Gregory Plumb, Ameet Talwalkar."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p4.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 04")),": ",i.a.createElement("b",null,"Xplique: A Deep Learning Explainability Toolbox.")," Thomas FEL, Lucas Hervier, David Vigouroux, Antonin Poche, Justin Plakoo, Remi Cadene, Mathieu Chalvidal, Julien Colin, Thibaut Boissin, Louis B\xe9thune, Agustin Picard, Claire NICODEME, Laurent Gardes, Gr\xe9gory Flandin, Thomas Serre."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p6.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 06")),": ",i.a.createElement("b",null,"HIVE: Evaluating the Human Interpretability of Visual Explanations.")," Sunnie S. Y. Kim, Nicole Meister, Vikram V. Ramaswamy, Ruth C Fong, Olga Russakovsky."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p10.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 10")),": ",i.a.createElement("b",null,"Give Users What They Want: Labeled Arrows.")," Severine Soltani, Michael Pazzani."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p11.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 11")),": ",i.a.createElement("b",null,"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic Filter Attention.")," Yu Yang, Seungbae Kim, Jungseock Joo."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p12.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 12")),": ",i.a.createElement("b",null,"HINT: Hierarchical Neuron Concept Explainer.")," Andong Wang, W.N. Lee, Xiaojuan Qi"),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p17.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 17")),": ",i.a.createElement("b",null,"Causality for Inherently Explainable Transformers: CAT-XPLAIN.")," Subash Khanal, Benjamin Brodie, Xin Xing, Ai-Ling Lin, Nathan Jacobs."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p18.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 18")),": ",i.a.createElement("b",null,"Ensembles for Improved Explanation of Image Classification.")," Aadil Ahamed, Kamran Alipour, Michael Pazzani, Sateesh Kumar."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p20.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 20")),": ",i.a.createElement("b",null,"Do learned representations respect causal relationships?")," Lan Wang, Vishnu Boddeti."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p21.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 21")),": ",i.a.createElement("b",null,"ELUDE: Generating Interpretable Explanations via a Decomposition into Labelled and Unlabelled features.")," Vikram V. Ramaswamy, Sunnie S. Y. Kim, Nicole Meister, Ruth C Fong, Olga Russakovsky."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p22.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 22")),": ",i.a.createElement("b",null,"Explaining Local Discrepancies between Image Classification Models.")," Thibault Laugel, Xavier Renard, Marcin Detyniecki."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p23.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 23")),": ",i.a.createElement("b",null,"CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations.")," Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p24.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 24")),": ",i.a.createElement("b",null,"Cycle-Consistent Counterfactuals by Latent Transformations.")," Saeed Khorram, Li Fuxin."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p26.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 26")),": ",i.a.createElement("b",null,"Subspace Based Visualization for Embedding Network.")," Xiaotong Liu, Abby Stylianou, Zeyu Zhang, Robert Pless."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"10:30 AM - 11:00 AM: ")," Invited Talk - ",i.a.createElement("b",null,"Antonio Torralba")," (MIT)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Interpreting and editing networks")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/opNPswunmfY"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"11:00 AM - 11:30 AM: ")," Invited Talk - ",i.a.createElement("b",null,"Yixin Wang")," (University of Michigan)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Representation Learning: A Causal Perspective")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data like images and text. Ideally, such a representation should capture non-spurious features of the data in an efficient way. It shall also be disentangled so that we can freely manipulate each of its dimensions. However, these desiderata are often intuitively defined and challenging to quantify or enforce. In this work, we take on a causal perspective of representation learning. We show how desiderata of representation learning can be formalized using counterfactual notions, which then enables algorithms that target efficient, non-spurious, and disentangled representations of data. We discuss the theoretical underpinnings of the algorithm and illustrate its empirical performance in both supervised and unsupervised representation learning. This is joint work with Michael Jordan."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://urldefense.com/v3/__https://yixinwang.github.io/papers/causal-rep-slides-public.pdf__;!!Bt8RZUm9aw!6HJjmzJvZjCXXLimgLDCFBHljTR3Q-GueFSoaY4aeHdW_S6ycHUDg8LnXUR9wiDHuFrVuvHNIlrzPALHSG8$"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," Presentation")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"11:30 AM - 12:00 PM: ")," Invited Talk - ",i.a.createElement("b",null,"Rich Caruana")," (MSR)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Glassbox Deep Learning with Neural Additive Models (NAMs)")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"Most interpretable machine learning methods depend on models trained with linear regression, decision lists, rules, or trees.  However it is possible to train interpretable models with DNNs by restricting the architecture to learn components that have reduced complexity and thus are easy to explain.  In this presentation I\u2019ll review NAMs --- Neural Additive Models --- where deep learning is used to train neural nets that are inherently interpretable yet highly accurate on tabular data.  Although these glassbox models are not designed for vision, I\u2019ll briefly discuss how they might be used to improve the intelligibility of some vision systems."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/cxc0zY0Btf4"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"12:00 PM - 12:50 PM: ")," Lunch Break"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"12:50 PM - 01:30 PM: ")," Spotlight Session 2 (",i.a.createElement("i",null,"8 min + 2 min QA"),")"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,"S 27: ",i.a.createElement("b",null,"A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information.")," Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil Bruce, Rick Wildes, Konstantinos G Derpanis."),i.a.createElement("li",null,"S 31: ",i.a.createElement("b",null,"Consistent Explanations by Contrastive Learning.")," Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, Hamed Pirsiavash."),i.a.createElement("li",null,"S 35: ",i.a.createElement("b",null,"OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data.")," David Schinagl, Georg Krispel, Horst Possegger, Peter M. Roth, Horst Bischof."),i.a.createElement("li",null,"S 54: ",i.a.createElement("b",null,"B-cos Networks: Alignment is All We Need for Interpretability.")," Moritz B\xf6hle, Mario Fritz, Bernt Schiele."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/KnMFDeMk9Us"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"01:30 PM - 02:00 PM: ")," Invited Talk - ",i.a.createElement("b",null,"Been Kim")," (Google Brain)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Bridging the representation gap between humans and machines: unique challenges in vision")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"Machines and humans are likely to understand the world differently. They may share some of their representational spaces, but not all. Bridging the gap between the two representational spaces is the key for the future ML in both developing and understanding them. While there have been many advances in bridging this gap in vision, there are unique challenges that we should keep in mind as we go forward. On the other hand, vision may offer unique (perhaps the only) ways for us to understand machine concepts. I will discuss some of our recent work that will help us discuss these topics."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/bKpxteeXSh4"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"02:00 PM - 02:30 PM: ")," Invited Talk - ",i.a.createElement("b",null,"Trevor Darrell")," (UC Berkeley)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"From Explainable to Advisable Learning")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"Explainable AI can augment or interpret existing AI models to allow them to be more transparent to human users and/or developers.  This can lead to greater acceptance and understanding of an AI agent's actions and capabilities.  XAI techniques also can lead to more performant systems, when an inverse explanation process is employed to provide advice to an AI agent.  In this talk I'll review recent progress towards explainable and advisable systems, including continual learning methods that learn to \"remember for the right reasons\", methods for dealing with heavily biased datasets based on attention advice, and recent work on language-based advice for visual classification.  By developing AI systems that can explain themselves, we find that we can use those same mechanisms to teach them to become better at what they need to do."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/74TE4NRiuaQ"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"02:30 PM - 03:10 PM: ")," Coffee Break + Poster Session 2"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p27.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 27")),": ",i.a.createElement("b",null,"A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information.")," Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil Bruce, Rick Wildes, Konstantinos G Derpanis."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p31.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 31")),": ",i.a.createElement("b",null,"Consistent Explanations by Contrastive Learning.")," Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, Hamed Pirsiavash."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p32.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 32")),": ",i.a.createElement("b",null,"Testing Explanation Algorithms on Transformers.")," Mingqi Jiang, Saeed Khorram, Li Fuxin."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p33.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 33")),": ",i.a.createElement("b",null,"FD-CAM: Improving Faithfulness and Discriminability of Visual Explanation for CNNs.")," Hui Li, Zihao Li, Rui Ma, Tieru Wu."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p34.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 34")),": ",i.a.createElement("b",null,"Exploring Concept Contribution Spatially: Hidden Layer Interpretation with Spatial Activation Concept Vector.")," Andong Wang, W.N. Lee."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p35.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 35")),": ",i.a.createElement("b",null,"OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data.")," David Schinagl, Georg Krispel, Horst Possegger, Peter M. Roth, Horst Bischof."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p36.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 36")),": ",i.a.createElement("b",null,"Pose Tutor: An Explainable System for Correction in the Wild.")," Bhat Dittakavi, Bharathi Callepalli, Sai Vikas Desai, Divyagna Bavikadi, Soumi Chakraborty, Nishant S Reddy, Ayon Sharma, Vineeth N Balasubramanian."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p39.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 39")),": ",i.a.createElement("b",null,"CheXplaining in Style: Counterfactual Explanations for Chest X-rays.")," Matan Atad, Vitalii Dmytrenko, Yitong Li, Xinyue Zhang, Matthias Keicher, Jan S. Kirschke, Bene Wiestler, Ashkan Khakzar, Nassir Navab."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p42.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 42")),": ",i.a.createElement("b",null,"Auditing Privacy Protection in Split Computing via Data-Free Model Inversion.")," Xin Dong, Hongxu Yin, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p44.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 44")),": ",i.a.createElement("b",null,"Towards ML Methods for Biodiversity: A Novel Wild Bee Dataset and Evaluations of XAI Methods for ML-Assisted Rare Species Annotations.")," Teodor Chiaburu, Felix Bie\xdfmann, Frank Hau\xdfer."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p46.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 46")),": ",i.a.createElement("b",null,"Spatial-temporal Concept based Explanation of 3D ConvNets.")," Ying Ji, Yu Wang, Kensaku Mori, Jien Kato."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p49.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 49")),": ",i.a.createElement("b",null,"Visual correspondence-based explanations improve human-AI team accuracy.")," Anh Nguyen, GIANG NGUYEN, Mohammad Reza Taesiri."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p50.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 50")),": ",i.a.createElement("b",null,"Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations.")," Ziyan Yang, Kushal Kafle, Franck Dernoncourt, Vicente Ordonez."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p52.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 52")),": ",i.a.createElement("b",null,"Gradient-weighted Class Activation Mapping for spatio temporal graph convolutional network.")," PRATYUSHA DAS, ANTONIO ORTEGA."),i.a.createElement("li",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/posters/p54.pdf"},i.a.createElement("b",null,i.a.createElement(K.a,{fontSize:"inherit"})," P 54")),": ",i.a.createElement("b",null,"B-cos Networks: Alignment is All We Need for Interpretability.")," Moritz B\xf6hle, Mario Fritz, Bernt Schiele."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"03:10 PM - 03:40 PM: ")," Invited Talk - ",i.a.createElement("b",null,"Pradeep Ravikumar")," (Carnegie Mellon University)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Objective criteria for explanations of machine learning models")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"Objective criteria to evaluate explanations of machine learning models are a critical ingredient in bringing greater rigor to the field of explainable artificial intelligence. We discuss three such criteria that each target different classes of explanations. In the first, targeted at real-valued feature importance explanations, we define a class of \u201cinfidelity\u201d measures that capture how well the explanations match the ML models. We show that instances of such infidelity minimizing explanations correspond to many popular recently proposed explanations and, moreover, can be shown to satisfy well-known game-theoretic axiomatic properties. In the second, targeted to feature set explanations, we define a robustness analysis-based criterion and show that deriving explainable feature sets based on the robustness criterion yields more qualitatively impressive explanations. Lastly, for sample explanations, we provide a decomposition-based representer criterion that allows us to provide very scalable and compelling classes of sample-based explanations. Joint with Chih-Kuan Yeh."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/lBiuROaEvnw"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"03:40 PM - 04:10 PM: ")," Invited Talk - ",i.a.createElement("b",null,"Serena Yeung")," (Stanford)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Multimodal vision-language models towards explainable medical computer vision")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"The COVID-19 pandemic and needs that arose were a test for real-world application of computer vision models in healthcare. In this talk I will first discuss how computer vision models that were developed during the pandemic fared, including with respect to explainability. I will then discuss learning cross-modal representation spaces towards explainability, in the context of medical imaging. Finally I will discuss work on more generally investigating the structure of cross-modal representation spaces, in particular the phenomenon of modality gap, and its implications."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/aPoDbpnpC7I"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"04:10 PM - 04:40 PM: ")," Invited Talk - ",i.a.createElement("b",null,"Hima Lakkaraju")," (Harvard University)"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("b",null,i.a.createElement("i",null,"Bringing Order to the Chaos: Probing the Disagreement Problem in XAI")))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/8ptuYPxxRbk"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"04:40 PM - 04:50 PM: ")," Break"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"04:50 PM - 05:50 PM: ")," Panel Discussion"),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,"We would like to initiate discussions across industry and academia that work on building interpretable and trustworthy systems useful to the broader vision community."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,i.a.createElement("b",null,"Panelists: ")," Antonio Torralba, Yixin Wang, Rich Caruana, Pradeep Ravikumar, Hima Lakkaraju."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("i",null,i.a.createElement("b",null,"Discussion lead by: ")," Filip Radenovic, Deepti Ghardiyaram, Devi Parikh."))),i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/wGPNW_H_z5w"},i.a.createElement("b",null,i.a.createElement(X.a,{fontSize:"inherit"})," Video Recording")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body1",align:"left"},i.a.createElement("b",null,"05:50 PM - 06:00 PM: ")," Closing Remarks - ",i.a.createElement("b",null,"Filip Radenovic")," (Meta AI)"))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left",id:"call_for_papers"},"Call for Papers")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"We welcome 2 page (including references) extended abstract submissions that showcase successful application of XAI methods on popular computer vision tasks, helping users better understand the results of a model. Moreover, we also welcome the submission of novel XAI for CV techniques, visualizations, and practical libraries.",i.a.createElement("br",null)," ",i.a.createElement("br",null),"Finally, we encourage submitting papers accepted in the CVPR 2022 main program, or in a relevant 2022 conference, in which case authors do not need to prepare extended abstract, but simply submit the camera-ready version of the accepted paper.",i.a.createElement("br",null)," ",i.a.createElement("br",null),"Note that, accepted extended abstracts will not be published in conjunction with the CVPR 2022 proceedings.")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Timeline")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"CMT portal opens on ",i.a.createElement("b",null,"Feb 2, 2022"),". ",i.a.createElement("br",null),"Submissions until ",i.a.createElement("b",null,"May 18, 2022"),". ",i.a.createElement("br",null),"Rolling acceptances from ",i.a.createElement("b",null,"May 1, 2022")," until ",i.a.createElement("b",null,"May 20, 2022"),". ",i.a.createElement("br",null),"Spotlight / Poster decisions on ",i.a.createElement("b",null,"May 23, 2022"),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Submission instructions")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"Extended Abstracts: ")," All submissions should be in the anonymized CVPR 2022 format, and submissions will be subjected to the double-blind review process. Submitted extended abstracts will be reviewed by 1-2 reviewers, with the organizing team serving as a program committee.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Papers accepted in a relevant 2022 conference: ")," Authors of papers accepted in the CVPR 2022 main program, or in a relevant 2022 conference, are encouraged to submit the camera-ready version and present their work at our workshop. Organizing team will review the relevance of submitted papers to the XAI4CV workshop.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Submissions")," can be done at ",i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://cmt3.research.microsoft.com/XAI4CV2022"},i.a.createElement("b",null,"https://cmt3.research.microsoft.com/XAI4CV2022")),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Attendance")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"Posters: ")," All accepted submissions will be invited to participate in an ",i.a.createElement("b",null,"in-person")," poster session at our workshop. Additionally, the authors will be asked to upload their posters which will be hosted on our webpage.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Spotlights: ")," We will pick the best 6 papers from among the submissions / main conference to be presented as spotlights. Presentations can either be ",i.a.createElement("b",null,"in-person")," or ",i.a.createElement("b",null,"pre-recorded"),".",i.a.createElement("br",null),i.a.createElement("br",null),"Abiding by the ",i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://cvpr2022.thecvf.com/registration"},i.a.createElement("b",null,"CVPR guidelines")),", all accepted papers ",i.a.createElement("b",null,"must be presented by one of the authors"),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Topics")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null," Building inherently interpretable CV models,"),i.a.createElement("li",null," Black-box CV model explanations visualized on test images and / or presented in a human interpretable language,"),i.a.createElement("li",null," Object classification / detection / segmentation model explanations,"),i.a.createElement("li",null," Action detection model explanations,"),i.a.createElement("li",null," VQA model explanations,"),i.a.createElement("li",null," Medical imaging model explanations,"),i.a.createElement("li",null," Human intervention and correctability in computer vision,"),i.a.createElement("li",null," Machine teaching via explanations,"),i.a.createElement("li",null," Constructing datasets for benchmarking explainability,"),i.a.createElement("li",null," Offline and online evaluation methods for explanations,"),i.a.createElement("li",null," Building practical libraries for explainable computer vision, and their integration with popular CV libraries.")))),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Organizers")),i.a.createElement(B,null),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Contact")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},"Email: ",i.a.createElement(C.a,{href:"mailto:filipradenovic@fb.com"},"filipradenovic@fb.com")))),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container})))))))}}]),t}(i.a.Component),J=Object(M.a)(function(e){return{content:{margin:"0 auto",marginTop:"1.5em"},root:{},gridItem:{padding:e.spacing(1.5)},sectionHeader:{marginTop:"0.15em"},container:{padding:e.spacing(2)},footer:{marginTop:"0.15em",fontSize:14},people:{margin:"0 auto",marginTop:"1.5em"}}})(U),Y=t(77),Z=Object(M.a)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:e.spacing(2)}}})(function(e){var a=[],t=4;a=e.people?e.people:Y.a,e.lgSize&&(t=e.lgSize);var n=a.map(function(a){var n=Math.random();return i.a.createElement(V.a,{item:!0,key:n,xs:12,sm:8,md:6,lg:t},i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(L.a,{className:e.classes.avatar,src:a.img_url})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(T.a,{variant:"subtitle1"},i.a.createElement("b",null,a.name))),i.a.createElement(T.a,{variant:"caption"},a.organization))});return i.a.createElement(V.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start"},n)}),Q=t(78),q=Object(M.a)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:e.spacing(2)}}})(function(e){var a=[],t=4;a=e.speakers?e.speakers:Q.a,e.lgSize&&(t=e.lgSize);var n=a.map(function(a){var n=Math.random();return i.a.createElement(V.a,{item:!0,key:n,xs:12,sm:8,md:6,lg:t},i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(L.a,{className:e.classes.avatar,src:a.img_url})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(T.a,{variant:"subtitle1"},i.a.createElement("b",null,a.name))),i.a.createElement(T.a,{variant:"caption"},a.organization))});return i.a.createElement(V.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start"},n)}),$=t(33),ee=t.n($),ae=t(18),te=t.n(ae),ne=function(e){Object(u.a)(t,e);var a=Object(p.a)(t);function t(){var e;Object(c.a)(this,t);for(var n=arguments.length,l=new Array(n),r=0;r<n;r++)l[r]=arguments[r];return(e=a.call.apply(a,[this].concat(l))).state={checked:!1},e}return Object(m.a)(t,[{key:"render",value:function(){var e=this.props.classes;return document.title="Workshop",i.a.createElement("div",{className:e.root},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:10,md:9,lg:9,className:e.content},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:12,lg:12},i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,md:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"center"},"The 2nd Explainable AI for Computer Vision (XAI4CV) Workshop at CVPR 2023")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Recording:")," ",i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://cvpr.thecvf.com/virtual/2023/workshop/18445"},"https://cvpr.thecvf.com/virtual/2023/workshop/18445"),i.a.createElement("br",null),i.a.createElement("b",null,"Feedback form:")," ",i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://tinyurl.com/XAI4CVatCVPR2023"},"https://tinyurl.com/XAI4CVatCVPR2023"),i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Date:")," Monday, June 19, 2023 ",i.a.createElement("br",null),i.a.createElement("b",null,"Location:")," West 121-122, Vancouver Convention Center, Vancouver, BC, Canada ",i.a.createElement("br",null))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Motivation")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"Explainability of computer vision systems is critical for people to effectively use and interact with them. This workshop seeks to contribute to the development of more explainable CV systems by: (1) initiating discussions across researchers and practitioners in academia and industry to identify successes, failures, and priorities in current XAI work; (2) examining the strengths, weaknesses, and underlying assumptions of proposed XAI methods and establish best practices in evaluation of these methods; and (3) discussing the various nuances of explainability and brainstorm ways to build explainable CV systems that benefit all involved stakeholders.")),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Schedule")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},"The schedule is in local time (PDT)."))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"09:15 AM - 09:30 AM: ")," Opening Remarks"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"09:30 AM - 10:00 AM: ")," Invited Talk 1: ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://www.atb-potsdam.de/en/about-us/team/staff-members/person/marina-hohne"},"Marina M.-C. H\xf6hne (n\xe9e Vidovic)")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"How Much Can I Trust You? Towards Understanding Neural Networks"),": In my presentation I will talk about the need to understand the AI models, which are often referred to as black box models. Due to correlations in the training data, the models might have learned artifacts, which lead to undesired behavior and might pose a risk especially in safety-critical applications. I will present DORA, a framework to automatically detect neurons that have learned spurious concepts, such as Clever Hans artifacts. Furthermore, I will show how we can improve local and global explanation methods by incorporating the uncertainty knowledge from Bayesian neural networks (BNN) into the explanations and how to quantitatively evaluate your explanations by using Quantus."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"10:00 AM - 10:30 AM: ")," Invited Talk 2: ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://arvindsatya.com/"},"Arvind Satyanarayan")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Model Saliency is an Abstraction of Model Behavior"),': Saliency methods are a commonly used class of interpretability techniques that calculate how important each input feature is to a model\'s output. However, a growing body of evaluative work questions how faithfully saliency methods represent model behavior, and the degree to which we should rely on them to explain model reasoning. In this talk, I will instead frame saliency maps as computational abstractions: views into model behavior that selectively preserve and necessarily sacrifice information in service of human-centric goals. With this framing, we can think of individual methods as lying in a broader "design space"\u2014a common research methodology from HCI and Visualization that allows us to systematically analyze tradeoffs. In doing so, downstream users can now make informed decisions, choosing a saliency method best suited for their specific needs and context. Moreover, we are able to identify gaps in the current landscape and opportunities for future research (e.g., task-specific methods, new evaluative metrics, etc.).'))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"10:30 AM - 11:00 AM: ")," Spotlight Session 1"),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P03"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P03_ManifoldHypothesis.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," The Manifold Hypothesis for Gradient-Based Explanations.")," Sebastian Bordt, Uddeshya Upadhyay, Zeynep Akata, Ulrike von Luxburg."),i.a.createElement("li",null,i.a.createElement("b",null,"P04"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P04_HierarchicalExplanations.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/Cic0IWWnSnA"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/DzO-1Pc2NQ4"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Hierarchical Explanations for Video Action Recognition.")," Sadaf Gulshad, Teng Long, Nanne van Noord."),i.a.createElement("li",null,i.a.createElement("b",null,"P09"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P09_SegXResCAM.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/QsxcVJQ2cmI"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Seg-XRes-CAM: Explaining Spatially Local Regions in Image Segmentation.")," Syed Nouman Hasany, Caroline Petitjean, Fabrice M\xe9riaudeau."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"11:00 AM - 12:00 PM: ")," Poster Session 1 @ West Exhibit Hall #203 - #241"),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P01"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P01_ODSmoothGrad.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/mifgW9vzfgQ"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," ODSmoothGrad: Generating Saliency Maps for Object Detectors.")," Chul Gwon, Steven C. Howell. "),i.a.createElement("li",null,i.a.createElement("b",null,"P02"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P02_SanityChecks.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/q5SwSM1Y_Tc"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Sanity Checks for Patch Visualisation in Prototype-based Image Classification.")," Romain Xu-Darme, Georges Qu\xe9not, Zakaria Chihani, Marie-Christine Rousset."),i.a.createElement("li",null,i.a.createElement("b",null,"P03"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P03_ManifoldHypothesis.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," The Manifold Hypothesis for Gradient-Based Explanations.")," Sebastian Bordt, Uddeshya Upadhyay, Zeynep Akata, Ulrike von Luxburg."),i.a.createElement("li",null,i.a.createElement("b",null,"P04"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P04_HierarchicalExplanations.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/Cic0IWWnSnA"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/DzO-1Pc2NQ4"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Hierarchical Explanations for Video Action Recognition.")," Sadaf Gulshad, Teng Long, Nanne van Noord."),i.a.createElement("li",null,i.a.createElement("b",null,"P05"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P05_ConfusionMatrix.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/nlZwTpZ8wdI"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null,"A Confusion Matrix for Evaluating Feature Attribution Methods.")," Anna Arias-Duart, Ettore Mariotti, Dario Garcia-Gasulla, Jose Maria Alonso-Moral."),i.a.createElement("li",null,i.a.createElement("b",null,"P06"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P06_Robustness.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/J1dyxo9eZpM"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Robustness of Visual Explanations to Common Data Augmentation Methods.")," Lenka T\u011btkov\xe1, Lars Kai Hansen."),i.a.createElement("li",null,i.a.createElement("b",null,"P07"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P07_ShortcutRemoval.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/hfA4yE4Z0Qo"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Localized Shortcut Removal.")," Nicolas M. M\xfcller*, Jochen Jacobs*, Jennifer Williams, Konstantin B\xf6ttinger."),i.a.createElement("li",null,i.a.createElement("b",null,"P08"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P08_MedicalImaging.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/sk3WLuUY3ho"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Towards Evaluating Explanations of Vision Transformers for Medical Imaging.")," Piotr Komorowski, Hubert Baniecki, Przemyslaw Biecek."),i.a.createElement("li",null,i.a.createElement("b",null,"P09"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P09_SegXResCAM.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/QsxcVJQ2cmI"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Seg-XRes-CAM: Explaining Spatially Local Regions in Image Segmentation.")," Syed Nouman Hasany, Caroline Petitjean, Fabrice M\xe9riaudeau."),i.a.createElement("li",null,i.a.createElement("b",null,"P10"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P10_MonocularCriteria.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/HvFCZLBOVg4"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Analyzing Results of Depth Estimation Models with Monocular Criteria.")," Jonas Theiner, Nils Nommensen, Jim Rhotert, Matthias Springstein, Eric M\xfcller-Budack, Ralph Ewerth."),i.a.createElement("li",null,i.a.createElement("b",null,"P11"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P11_Text2Concept.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/_70P9hBB-Fs"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P11_Text2Concept_poster.pdf"}," ",i.a.createElement(ee.a,{fontSize:"inherit"})),i.a.createElement("b",null," Text2Concept: Concept Activation Vectors Directly From Text.")," Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, Soheil Feizi."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"12:00 PM - 01:30 PM: ")," Lunch Social @ West Ballrooms"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"01:30 PM - 02:00 PM: ")," Invited Talk 3: ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://www.cs.unc.edu/~mbansal/"},"Mohit Bansal")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Interpretable Image Generation+Evaluation and Right-for-the-Right-Reason Explanation Supervision"),": In this talk, we will first discuss interpretable visual programming frameworks for text-to-image (T2I) generation and evaluation. VPGen decomposes T2I generation into three steps: object/count generation, layout generation, and image generation, providing stronger counts/spatial relations/scales control than end-to-end models and leveraging the world knowledge of pretrained LMs to go beyond predefined object classes. Next, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results, resulting in a more human-correlated evaluation. In the second part of the talk, we will discuss how to make model explanation supervision 'meaningfully' improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: sufficiency, uncertainty, invariance, and plausibility. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (i.e., whether explanations truly represent the model's internal reasoning)."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"02:00 PM - 02:30 PM: ")," Invited Talk 4: ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"http://qveraliao.com/"},"Q. Vera Liao")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Towards Human-Compatible Explainable AI"),": While a vast collection of explainable AI (XAI) techniques have been developed in recent years, human-computer interaction (HCI) studies have found mixed results of their effectiveness, even pitfalls, in helping people work with AI systems. There is often a lack of compatibility with, and to begin with, a lack of understanding of, how people process and make use of explanations. In this talk, I will first draw on our own work and the broader HCI research to provide a more principled understanding of how people use AI explanations, focusing on the context of AI-assisted decision support. I will then suggest a path to more human-compatible XAI by drawing inspiration from human explanation behaviors, and encourage the community to pay more attention to the communication of explanations."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"02:30 PM - 03:00 PM: ")," Spotlight Session 2"),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P12"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P12_CAVLI.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/D0a7gRyjCeU"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P12_CAVLI_poster.pdf"}," ",i.a.createElement(ee.a,{fontSize:"inherit"})),i.a.createElement("b",null," CAVLI - Using Image Associations to Produce Local Concept-based Explanations.")," Pushkar Shukla, Sushil Bharati, Matthew Turk."),i.a.createElement("li",null,i.a.createElement("b",null,"P16"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P16_SharedInterestSometimes.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/C180ttgD-18"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Shared Interest...Sometimes: Understanding the Alignment between Human Perception, Vision Architectures, and Saliency Map Techniques.")," Katelyn Morrison, Ankita Mehra, Adam Perer."),i.a.createElement("li",null,i.a.createElement("b",null,"P20"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P20_PIPNet.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/GfQQFQ62SLU"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification.")," Meike Nauta, J\xf6rg Schl\xf6tterer, Maurice Van Keulen, Christin Seifert."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"03:00 PM - 04:00 PM: ")," Poster Session 2 @ West Exhibit Hall #203 - #241"),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P12"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P12_CAVLI.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/D0a7gRyjCeU"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P12_CAVLI_poster.pdf"}," ",i.a.createElement(ee.a,{fontSize:"inherit"})),i.a.createElement("b",null," CAVLI - Using Image Associations to Produce Local Concept-based Explanations.")," Pushkar Shukla, Sushil Bharati, Matthew Turk."),i.a.createElement("li",null,i.a.createElement("b",null,"P13"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P13_VisionDiffMask.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/YVtpcq6ceVI"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Vision DiffMask: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking.")," Angelos Nalmpantis*, Apostolos Panagiotopoulos*, John Gkountouras*, Konstantinos Papakostas, Wilker Aziz."),i.a.createElement("li",null,i.a.createElement("b",null,"P14"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P14_TaskAgnostic.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/9zagQAm4krs"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Ante-Hoc Generation of Task-Agnostic Interpretation Maps.")," Akash Guna R T, Raul Benitez, Sikha O K."),i.a.createElement("li",null,i.a.createElement("b",null,"P15"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P15_Disentangling.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/wOWUfS11HV8"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Disentangling Neuron Representations with Concept Vectors.")," Laura O'Mahony, Vincent Andrearczyk, Henning M\xfcller, Mara Graziani."),i.a.createElement("li",null,i.a.createElement("b",null,"P16"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P16_SharedInterestSometimes.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/C180ttgD-18"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Shared Interest...Sometimes: Understanding the Alignment between Human Perception, Vision Architectures, and Saliency Map Techniques.")," Katelyn Morrison, Ankita Mehra, Adam Perer."),i.a.createElement("li",null,i.a.createElement("b",null,"P17"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P17_ZEBRA.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/u_m2tos100A"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," ZEBRA: Explaining Rare Cases through Outlying Interpretable Concepts.")," Pedro Madeira, Andr\xe9 Carreiro, Alex Gaudio, Lu\xeds Rosado, Filipe Soares, Asim Smailagic."),i.a.createElement("li",null,i.a.createElement("b",null,"P18"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P18_ChestXray.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/iT90lH14g4Y"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," The Effect of Counterfactuals on Reading Chest X-rays.")," Joseph Paul Cohen, Rupert Brooks, Sovann En, Evan Zucker, Anuj Pareek, Matthew Lungren, Akshay Chaudhari."),i.a.createElement("li",null,i.a.createElement("b",null,"P19"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P19_SOXAI.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability.")," E. Zhixuan Zeng, Hayden Gunraj, Sheldon Fernandez, Alexander Wong."),i.a.createElement("li",null,i.a.createElement("b",null,"P20"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P20_PIPNet.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/GfQQFQ62SLU"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification.")," Meike Nauta, J\xf6rg Schl\xf6tterer, Maurice Van Keulen, Christin Seifert."),i.a.createElement("li",null,i.a.createElement("b",null,"P21"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P21_CRAFT.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," CRAFT: Concept Recursive Activation FacTorization for Explainability.")," Thomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux, Julien Colin, R\xe9mi Cad\xe8ne, Thomas Serre."),i.a.createElement("li",null,i.a.createElement("b",null,"P22"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P22_MACO.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization.")," Thomas Fel, Thibaut Boissin, Victor Boutin, Agustin Picard, Paul Novello, Julien Colin, Drew Linsley, Tom Rousseau, R\xe9mi Cad\xe8ne, Laurent Gardes, Thomas Serre."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"04:00 PM - 04:30 PM: ")," Invited Talk 5: ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://baulab.info/"},"David Bau")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Causal Explanations"),": If every explanation of a neural network is just an approximation to the actual network, then what makes a good explanation? I will talk about the role of causal models, mechanistic interpretability, and model editing, and I will talk about how a good explanation is falsifiable.  Then I will share a story about a recent piece of research where we missed a key part of the explanation, and another researcher used the same causal methods to find a better explanation for the same network."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"04:30 PM - 05:00 PM: ")," Invited Talk 6: ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://ai.sony/people/Alice-Xiang/"},"Alice Xiang")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Being 'Seen' vs. 'Mis-Seen': Tensions between Privacy and Fairness in Computer Vision"),": The rise of facial recognition and related computer vision technologies has been met with growing anxiety over the potential for AI to create mass surveillance systems and further entrench societal biases. These concerns have led to calls for greater privacy protections and fairer, less biased algorithms. An under-appreciated tension, however, is that privacy protections and bias mitigation efforts can sometimes conflict in the context of AI. Reducing bias in human-centric computer vision systems (\u201cHCCV\u201d), including facial recognition, can involve collecting large, diverse, and candid image datasets, which can run counter to privacy protections. In this talk, I discuss this tension between privacy and fairness in the context of algorithmic bias mitigation for HCCV, review the strategies proposed for resolving this tension, and examine the implications of a right not to be disproportionately \u201cmis-seen\u201d by AI, in contrast to regulations around what data should remain \u201cunseen.\u201d De-tethering these notions (being seen versus unseen versus mis-seen) can help clarify what rights relevant laws and policies should seek to protect."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"05:00 PM - 05:15 PM: ")," Closing Remarks"))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Invited Speakers")),i.a.createElement(q,null),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Accepted Papers")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Proceedings Track")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P01"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P01_ODSmoothGrad.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/mifgW9vzfgQ"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," ODSmoothGrad: Generating Saliency Maps for Object Detectors.")," Chul Gwon, Steven C. Howell. "),i.a.createElement("li",null,i.a.createElement("b",null,"P02"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P02_SanityChecks.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/q5SwSM1Y_Tc"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Sanity Checks for Patch Visualisation in Prototype-based Image Classification.")," Romain Xu-Darme, Georges Qu\xe9not, Zakaria Chihani, Marie-Christine Rousset."),i.a.createElement("li",null,i.a.createElement("b",null,"P03"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P03_ManifoldHypothesis.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," The Manifold Hypothesis for Gradient-Based Explanations.")," Sebastian Bordt, Uddeshya Upadhyay, Zeynep Akata, Ulrike von Luxburg."),i.a.createElement("li",null,i.a.createElement("b",null,"P04"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P04_HierarchicalExplanations.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/Cic0IWWnSnA"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/DzO-1Pc2NQ4"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Hierarchical Explanations for Video Action Recognition.")," Sadaf Gulshad, Teng Long, Nanne van Noord."),i.a.createElement("li",null,i.a.createElement("b",null,"P05"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P05_ConfusionMatrix.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/nlZwTpZ8wdI"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null,"A Confusion Matrix for Evaluating Feature Attribution Methods.")," Anna Arias-Duart, Ettore Mariotti, Dario Garcia-Gasulla, Jose Maria Alonso-Moral."),i.a.createElement("li",null,i.a.createElement("b",null,"P06"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P06_Robustness.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/J1dyxo9eZpM"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Robustness of Visual Explanations to Common Data Augmentation Methods.")," Lenka T\u011btkov\xe1, Lars Kai Hansen."),i.a.createElement("li",null,i.a.createElement("b",null,"P07"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P07_ShortcutRemoval.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/hfA4yE4Z0Qo"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Localized Shortcut Removal.")," Nicolas M. M\xfcller*, Jochen Jacobs*, Jennifer Williams, Konstantin B\xf6ttinger."),i.a.createElement("li",null,i.a.createElement("b",null,"P08"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P08_MedicalImaging.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/sk3WLuUY3ho"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Towards Evaluating Explanations of Vision Transformers for Medical Imaging.")," Piotr Komorowski, Hubert Baniecki, Przemyslaw Biecek."),i.a.createElement("li",null,i.a.createElement("b",null,"P09"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P09_SegXResCAM.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/QsxcVJQ2cmI"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Seg-XRes-CAM: Explaining Spatially Local Regions in Image Segmentation.")," Syed Nouman Hasany, Caroline Petitjean, Fabrice M\xe9riaudeau."),i.a.createElement("li",null,i.a.createElement("b",null,"P10"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P10_MonocularCriteria.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/HvFCZLBOVg4"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Analyzing Results of Depth Estimation Models with Monocular Criteria.")," Jonas Theiner, Nils Nommensen, Jim Rhotert, Matthias Springstein, Eric M\xfcller-Budack, Ralph Ewerth."),i.a.createElement("li",null,i.a.createElement("b",null,"P11"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P11_Text2Concept.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/_70P9hBB-Fs"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P11_Text2Concept_poster.pdf"}," ",i.a.createElement(ee.a,{fontSize:"inherit"})),i.a.createElement("b",null," Text2Concept: Concept Activation Vectors Directly From Text.")," Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, Soheil Feizi."),i.a.createElement("li",null,i.a.createElement("b",null,"P12"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P12_CAVLI.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/D0a7gRyjCeU"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P12_CAVLI_poster.pdf"}," ",i.a.createElement(ee.a,{fontSize:"inherit"})),i.a.createElement("b",null," CAVLI - Using Image Associations to Produce Local Concept-based Explanations.")," Pushkar Shukla, Sushil Bharati, Matthew Turk."),i.a.createElement("li",null,i.a.createElement("b",null,"P13"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P13_VisionDiffMask.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/YVtpcq6ceVI"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Vision DiffMask: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking.")," Angelos Nalmpantis*, Apostolos Panagiotopoulos*, John Gkountouras*, Konstantinos Papakostas, Wilker Aziz."),i.a.createElement("li",null,i.a.createElement("b",null,"P14"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P14_TaskAgnostic.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/9zagQAm4krs"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Ante-Hoc Generation of Task-Agnostic Interpretation Maps.")," Akash Guna R T, Raul Benitez, Sikha O K."),i.a.createElement("li",null,i.a.createElement("b",null,"P15"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P15_Disentangling.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/wOWUfS11HV8"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," Disentangling Neuron Representations with Concept Vectors.")," Laura O'Mahony, Vincent Andrearczyk, Henning M\xfcller, Mara Graziani."),i.a.createElement("li",null,i.a.createElement("b",null,"P16"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P16_SharedInterestSometimes.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/C180ttgD-18"}," ",i.a.createElement(te.a,{fontSize:"inherit"})),i.a.createElement("b",null," Shared Interest...Sometimes: Understanding the Alignment between Human Perception, Vision Architectures, and Saliency Map Techniques.")," Katelyn Morrison, Ankita Mehra, Adam Perer."),i.a.createElement("li",null,i.a.createElement("b",null,"P17"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P17_ZEBRA.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/u_m2tos100A"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," ZEBRA: Explaining Rare Cases through Outlying Interpretable Concepts.")," Pedro Madeira, Andr\xe9 Carreiro, Alex Gaudio, Lu\xeds Rosado, Filipe Soares, Asim Smailagic.")))),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Non-proceedings Track")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P18"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P18_ChestXray.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/iT90lH14g4Y"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," The Effect of Counterfactuals on Reading Chest X-rays.")," Joseph Paul Cohen, Rupert Brooks, Sovann En, Evan Zucker, Anuj Pareek, Matthew Lungren, Akshay Chaudhari."),i.a.createElement("li",null,i.a.createElement("b",null,"P19"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P19_SOXAI.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Explaining Explainability: Towards Deeper Actionable Insights into Deep Learning through Second-order Explainability.")," E. Zhixuan Zeng, Hayden Gunraj, Sheldon Fernandez, Alexander Wong."),i.a.createElement("li",null,i.a.createElement("b",null,"P20"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P20_PIPNet.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"https://youtu.be/GfQQFQ62SLU"}," ",i.a.createElement(X.a,{fontSize:"inherit"})),i.a.createElement("b",null," PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification.")," Meike Nauta, J\xf6rg Schl\xf6tterer, Maurice Van Keulen, Christin Seifert."),i.a.createElement("li",null,i.a.createElement("b",null,"P21"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P21_CRAFT.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," CRAFT: Concept Recursive Activation FacTorization for Explainability.")," Thomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux, Julien Colin, R\xe9mi Cad\xe8ne, Thomas Serre."),i.a.createElement("li",null,i.a.createElement("b",null,"P22"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2023/P22_MACO.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization.")," Thomas Fel, Thibaut Boissin, Victor Boutin, Agustin Picard, Paul Novello, Julien Colin, Drew Linsley, Tom Rousseau, R\xe9mi Cad\xe8ne, Laurent Gardes, Thomas Serre.")))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Organizers")),i.a.createElement(Z,null),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Program Committee")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"We thank our wonderful program committee members who made this workshop possible!"),i.a.createElement("br",null),i.a.createElement("br",null),"Abhimanyu Dubey, Andrew Elliott, Angelina Wang, Anirban Sarkar, Anmol Kalia, Arijit Ray, Chenyang Zhao, Chhavi Yadav, Chirag Agarwal, David Schinagl, Deepti Ghadiyaram, Devon Ulrich, Donald Honeycutt, Eunji Kim, Filip Radenovic, Giang Nguyen, Haofan Wang, Hubert Baniecki, Indu Panigrahi, Itana Bulatovic, Jae Myung Kim, Jihoon Chung, Jonathan Donnelly, Joseph Paul Cohen, Julien Colin, Kira Vinogradova, Lan Wang, Matthew Kowal, Mehdi Nourelahi, Mert Yuksekgonul, Moritz B\xf6hle, Peter Hase, Qi Li, Rama Kovvuri, Ruth Fong, Satyapriya Krishna, Sunnie S. Y. Kim, Thomas Fel, Vikram V. Ramaswamy, Vipin Pillai, William Yang, Xinran Liang"))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Contact")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},"Email: ",i.a.createElement(C.a,{href:"mailto:xai4cv2023@googlegroups.com"},"xai4cv2023@googlegroups.com")))),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"(Closed) Call for Papers & Demos")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"We welcome paper and demo submissions.",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Papers")," should describe high-quality, original research. Contributions can include novel XAI methods; applications of existing methods on new domains, models, and tasks; evaluation or analysis of existing methods; and practical toolboxes."),i.a.createElement("li",null,i.a.createElement("b",null,"Demos")," should consist of static or interactive presentations of XAI for CV models and tasks, accompanied by a description. Contributions can include visualizations, explanations, and explorations of novel XAI systems; novel visualizations, explanations, and explorations of existing XAI systems; studies of how different  visualizations, explanations, and explorations of XAI systems are perceived by people; among others.")),"We have two tracks of submissions.",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Proceedings track:")," We welcome ",i.a.createElement("b",null,"max 4-page")," submissions of papers and demos. Submissions accepted to this track ",i.a.createElement("b",null,"will be published")," in the CVPR 2023 workshop proceedings."),i.a.createElement("li",null,i.a.createElement("b",null,"Non-proceedings track:")," We welcome ",i.a.createElement("b",null,"max 2-page"),' submissions (commonly referred to as "extended abstracts") of papers and demos. For the non-proceedings track, we encourage submissions of published or accepted work (e.g., papers and demos accepted to the CVPR 2023 main program). Submissions accepted to this track ',i.a.createElement("b",null,"will ",i.a.createElement("i",null,"not")," be published")," in the CVPR 2023 workshop proceedings.")))),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Timeline")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"Proceedings track")," ",i.a.createElement("br",null),"Submissions due ",i.a.createElement("b",null,i.a.createElement("strike",null,"March 14, 2023 (5pm PT)")," March 17, 2023 (5pm PT)"),". ",i.a.createElement("br",null),"Decisions on ",i.a.createElement("b",null,i.a.createElement("strike",null,"April 1, 2023")," April 4, 2023"),". ",i.a.createElement("br",null),"Camera-ready due on ",i.a.createElement("b",null,i.a.createElement("strike",null,"April 8, 2023 (5pm PT)")," April 13, 2023 (5pm PT)"),".",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Non-proceedings track")," ",i.a.createElement("br",null),"Submissions due ",i.a.createElement("b",null,"May 19, 2023 (5pm PT)"),". ",i.a.createElement("br",null),"Rolling decisions from ",i.a.createElement("b",null,"May 1, 2023")," to ",i.a.createElement("b",null,"May 25, 2023"),". ",i.a.createElement("br",null),"Camera-ready due on ",i.a.createElement("b",null,"June 8, 2023 (5pm PT)"),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Submission instructions")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"All submissions should be in the ",i.a.createElement("b",null,"anonymized")," CVPR 2023 format available at ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://cvpr.thecvf.com/Conferences/2023/AuthorGuidelines"},i.a.createElement("b",null,"https://cvpr.thecvf.com/Conferences/2023/AuthorGuidelines")),".",i.a.createElement("br",null),"The page limits ",i.a.createElement("b",null,"do not")," include references.",i.a.createElement("br",null),"Submissions can be done at ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://cmt3.research.microsoft.com/XAI4CV2023"},i.a.createElement("b",null,"https://cmt3.research.microsoft.com/XAI4CV2023")),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Attendance & Presentation")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"Posters: ")," All accepted submissions will be invited to participate in an ",i.a.createElement("b",null,"in-person")," poster session at our workshop. Additionally, the authors will be asked to upload their posters which will be hosted on our webpage.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Spotlights: ")," We will pick 6-8 works among the submissions to be presented as spotlights. Presentations can either be ",i.a.createElement("b",null,"in-person")," or ",i.a.createElement("b",null,"pre-recorded"),".",i.a.createElement("br",null),i.a.createElement("br",null),"Abiding by the ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://cvpr2023.thecvf.com/Conferences/2023/Pricing2"},i.a.createElement("b",null,"CVPR guidelines")),", all accepted papers ",i.a.createElement("b",null,"must be presented by one of the authors"),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Topics")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,"Interpretable-by-design CV models"),i.a.createElement("li",null,"Post-hoc explanations of CV models"),i.a.createElement("li",null,"Evaluation and analysis of XAI for CV"),i.a.createElement("li",null,"Applications of XAI for CV"),i.a.createElement("li",null,"Methods for new interactions with CV models (e.g., debugging, editing)"),i.a.createElement("li",null,"Multimodal XAI, including both multimodal explanations of CV models and (unimodal) explanations of multimodal models"),i.a.createElement("li",null,"Datasets for developing and evaluating XAI for CV"),i.a.createElement("li",null,"Visualizations and toolboxes for XAI for CV"),i.a.createElement("li",null,"Human-centered XAI for CV (e.g., human evaluations, qualitative studies)"),i.a.createElement("li",null,"Studies of XAI for CV and related topics (e.g., fairness, transparency, interpretability, trust)")))),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container})))))))}}]),t}(i.a.Component),le=Object(M.a)(function(e){return{content:{margin:"0 auto",marginTop:"1.5em"},root:{},gridItem:{padding:e.spacing(1.5)},sectionHeader:{marginTop:"0.15em"},container:{padding:e.spacing(2)},footer:{marginTop:"0.15em",fontSize:14},people:{margin:"0 auto",marginTop:"1.5em"}}})(ne),re=t(79),ie=Object(M.a)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:e.spacing(2)}}})(function(e){var a=[],t=4;a=e.people?e.people:re.a,e.lgSize&&(t=e.lgSize);var n=a.map(function(a){var n=Math.random();return i.a.createElement(V.a,{item:!0,key:n,xs:12,sm:8,md:6,lg:t},i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(L.a,{className:e.classes.avatar,src:a.img_url})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(T.a,{variant:"subtitle1"},i.a.createElement("b",null,a.name))),i.a.createElement(T.a,{variant:"caption"},a.organization))});return i.a.createElement(V.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start"},n)}),se=t(80),oe=Object(M.a)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:e.spacing(2)}}})(function(e){var a=[],t=4;a=e.speakers?e.speakers:se.a,e.lgSize&&(t=e.lgSize);var n=a.map(function(a){var n=Math.random();return i.a.createElement(V.a,{item:!0,key:n,xs:12,sm:8,md:6,lg:t},i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(L.a,{className:e.classes.avatar,src:a.img_url})),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:a.website},i.a.createElement(T.a,{variant:"subtitle1"},i.a.createElement("b",null,a.name))),i.a.createElement(T.a,{variant:"caption"},a.organization))});return i.a.createElement(V.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start"},n)}),ce=function(e){Object(u.a)(t,e);var a=Object(p.a)(t);function t(){var e;Object(c.a)(this,t);for(var n=arguments.length,l=new Array(n),r=0;r<n;r++)l[r]=arguments[r];return(e=a.call.apply(a,[this].concat(l))).state={checked:!1},e}return Object(m.a)(t,[{key:"render",value:function(){var e=this.props.classes;return document.title="Workshop",i.a.createElement("div",{className:e.root},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:10,md:9,lg:9,className:e.content},i.a.createElement(V.a,{container:!0,justify:"center",alignContent:"center"},i.a.createElement(V.a,{item:!0,xs:12,lg:12},i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,md:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"center"},"The 3rd Explainable AI for Computer Vision (XAI4CV) Workshop at CVPR 2024")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Date:")," Tuesday, June 18, 2024 ",i.a.createElement("br",null),i.a.createElement("b",null,"Location:")," Arch 2A, Seattle Convention Center, Seattle, WA, USA ",i.a.createElement("br",null))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Motivation")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"Explainability of computer vision systems is critical for people to effectively use and interact with them. This workshop seeks to contribute to the development of more explainable CV systems by: (1) initiating discussions across researchers and practitioners in academia and industry to identify successes, failures, and priorities in current XAI work; (2) examining the strengths, weaknesses, and underlying assumptions of proposed XAI methods and establish best practices in evaluation of these methods; and (3) discussing the various nuances of explainability and brainstorm ways to build explainable CV systems that benefit all involved stakeholders.")),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Schedule")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},"The schedule is in local time (PDT)."))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"09:15 AM - 09:30 AM: ")," Opening Remarks"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"09:30 AM - 10:00 AM: ")," Invited Talk 1: \xa0",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele"},"Bernt Schiele")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Inherent Interpretability for Deep Learning in Computer Vision"),": Computer Vision has been revolutionized by Machine Learning and in particular Deep Learning. End-to-end trainable models often allow to achieve top performance across a wide range of computer vision tasks and settings. While recent progress is remarkable, current deep learning models are hard to interpret. In this talk discuss a new class of neural networks which are performant image classifiers with a high degree of inherent interpretability. In particular, these novel networks perform classification through a series of input-dependent linear transformations, that outperform existing attribution methods both quantitatively as well as qualitatively."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"10:00 AM - 10:30 AM: ")," Invited Talk 2: \xa0",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://ml-research.github.io/people/kkersting/"},"Kristian Kersting")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Reasonable Artificial Intelligence"),": To understand how to get (generative) AI right, we need to know how it can go wrong but also make assumptions on what is \u201cright\u201d and \u201cwrong.\u201d Consequently, I will argue that we need both explainable & interpretable AI and illustrate this using several examples."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"10:30 AM - 11:00 AM: ")," Spotlight Session 1"),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P08"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"assets/papers2024/P08.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Recent Trends, Challenges, and Limitations of Explainable AI in Remote Sensing.")," Adrian H\xf6hl*, Ivica Obadic*, Miguel-\xc1ngel Fern\xe1ndez-Torres, Dario Oliveira, Xiao Xiang Zhu."),i.a.createElement("li",null,i.a.createElement("b",null,"P02"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"assets/papers2024/P02.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Spatial Sensitive Grad-CAM++: Improved Visual Explanation for Object Detectors via Weighted Combination of Gradient Map.")," Toshinori Yamauchi."),i.a.createElement("li",null,i.a.createElement("b",null,"P04"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P04.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Exploring Explainability in Video Action Recognition.")," Avinab Saha*, Shashank Gupta*, Sravan Kumar Ankireddy*, Karl Chahine, Joydeep Ghosh."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"11:00 AM - 12:00 PM: ")," Poster Session 1"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"12:00 PM - 01:30 PM: ")," Lunch Social"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"01:30 PM - 02:00 PM: ")," Invited Talk 3: \xa0",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://eecs.uq.edu.au/profile/9477/tim-miller"},"Tim Miller")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Human-Centered Counterfactual Explanations for Image Classification"),": In this talk, I\u2019ll discuss some foundations of explainable AI based on social science research, with a particular focus on contrastive and counterfactual explanations. I will discuss some work in applying these ideas to image classification, in which we use concept-based explainers to generate counterfactuals orders of magnitude more quickly than existing image counterfactuals explanations, while also improving the interpretability/understandability of these."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"02:00 PM - 02:30 PM: ")," Invited Talk 4: \xa0",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://aims.cs.washington.edu/su-in-lee"},"Su-In Lee")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Explainable AI for Clinical AI Auditing")))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"02:30 PM - 03:00 PM: ")," Spotlight Session 2"),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P11"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P11.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits.")," Maximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek, Sebastian Lapuschkin."),i.a.createElement("li",null,i.a.createElement("b",null,"P13"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P13.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Quantifying Explainability with Multi-Scale Gaussian Mixture Models.")," Anthony Rhodes, Yali Bian, Ilke Demir."),i.a.createElement("li",null,i.a.createElement("b",null,"P05"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P05.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null,"LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models.")," Gabriela Ben Melech Stan*, Estelle Aflalo*, Raanan Yehezkel Rohekar*, Anahita Bhiwandiwalla*, Shao-Yen Tseng*, Matthew Lyle Olson*, Yaniv Gurwicz*, Chenfei Wu, Nan Duan, Vasudev Lal."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"03:00 PM - 04:00 PM: ")," Poster Session 2"))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"04:00 PM - 04:30 PM: ")," Invited Talk 5: \xa0",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://www.cs.ubc.ca/~lsigal/"},"Leonid Signal")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Understanding, Control and Debiasing of Text-to-Image Models"),": Text-to-image (TTI) diffusion models, such as Latent Diffusion, have emerged as a powerful class of generative models capable of generating stunning visuals conditioned on a text specification of the intended content. However, the quality of the textual prompts provided to these models ultimately determines the faithfulness of the generated content. Further, the \u201cquality\u201d of the prompt is both the function of the linguistic expression of the user\u2019s desire and the (generally unknown) distribution over the text-image pairs which were used to train the model in the first place. To greatly simplify the task of \u201cprompt engineering\u201d, we propose an intuitive prompt inversion approach which allows optimization of the prompt that is most likely to lead to a given example visual. This enables much simpler generation and manipulation of image content by direct editing (or concatenation) of inverted text prompts. Secondarily, TTI models have also been shown to suffer from harmful biases, including exaggerated societal biases (e.g., gender, ethnicity), as well as incidental correlations that limit such model\u2019s ability to generate more diverse imagery. I will subsequently talk about an approach we developed to study and quantify a broad spectrum of biases, for any TTI model and for any prompt, using counterfactual reasoning -- we call this framework TIBET. Our approach automatically identifies potential biases that might be relevant to the given prompt, and measures those biases. We show that our method is uniquely capable of explaining complex multi-dimensional biases through semantic concepts, as well as mitigating identified biases using existing approaches. This work is a collaboration between UBC, TTI-C, Carleton and UPenn."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"04:30 PM - 05:00 PM: ")," Invited Talk 6: \xa0",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://people.iith.ac.in/vineethnb/"},"Vineeth N Balasubramanian")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Moving beyond an Afterthought: Toward Learning via Explanations"),": The growing demand for explainability  in AI and machine learning models has largely been addressed with post-hoc methods that explain a model trained a priori.  While these methods have been valuable, this talk advocates for a shift towards incorporating  explainability directly into the learning process. We will discuss our recent work towards this objective from two perspectives: (i) integrating interpretability during model training in popular vision models; and (ii) integrating and maintaining causal attributions using domain knowledge in neural network models during the training process.  Presented at prominent venues like CVPR and ICML, our work exemplifies our goal to advance explainable AI systems from prediction-based learning to explanation-based learning."))))),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:12,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"05:00 PM - 05:15 PM: ")," Closing Remarks"))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Invited Speakers")),i.a.createElement(oe,null),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Accepted Papers")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Proceedings Track")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P01"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P01.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," ReciproCAM: Lightweight Gradient-free Class Activation Map for Post-hoc Explanations.")," Seok-Yong Byun, Wonju Lee. "),i.a.createElement("li",null,i.a.createElement("b",null,"P02"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P02.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Spatial Sensitive Grad-CAM++: Improved Visual Explanation for Object Detectors via Weighted Combination of Gradient Map.")," Toshinori Yamauchi."),i.a.createElement("li",null,i.a.createElement("b",null,"P03"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P03.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Allowing Humans to Interactively Guide Machines Where to Look Does Not Always Improve Human-AI Team\u2019s Classification Accuracy.")," Giang Nguyen*, Mohammad Reza Taesiri*, Sunnie S. Y. Kim, Anh Totti Nguyen."),i.a.createElement("li",null,i.a.createElement("b",null,"P04"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P04.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Exploring Explainability in Video Action Recognition.")," Avinab Saha*, Shashank Gupta*, Sravan Kumar Ankireddy*, Karl Chahine, Joydeep Ghosh."),i.a.createElement("li",null,i.a.createElement("b",null,"P05"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P05.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null,"LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models.")," Gabriela Ben Melech Stan*, Estelle Aflalo*, Raanan Yehezkel Rohekar*, Anahita Bhiwandiwalla*, Shao-Yen Tseng*, Matthew Lyle Olson*, Yaniv Gurwicz*, Chenfei Wu, Nan Duan, Vasudev Lal."),i.a.createElement("li",null,i.a.createElement("b",null,"P06"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P06.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Interactive Visual Feature Search.")," Devon Ulrich, Ruth Fong."),i.a.createElement("li",null,i.a.createElement("b",null,"P07"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P07.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Explaining Models Relating Objects and Privacy.")," Alessio Xompero, Myriam Bontonou, Jean-Michel Arbona, Emmanouil Benetos, Andrea Cavallaro."),i.a.createElement("li",null,i.a.createElement("b",null,"P08"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P08.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Recent Trends, Challenges, and Limitations of Explainable AI in Remote Sensing.")," Adrian H\xf6hl*, Ivica Obadic*, Miguel-\xc1ngel Fern\xe1ndez-Torres, Dario Oliveira, Xiao Xiang Zhu."),i.a.createElement("li",null,i.a.createElement("b",null,"P09"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P09.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," CA-Stream: Attention-based Pooling for Interpretable Image Recognition.")," Felipe Torres, Hanwei Zhang, Ronan Sicre, St\xe9phane Ayache, Yannis Avrithis."),i.a.createElement("li",null,i.a.createElement("b",null,"P10"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P10.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," SUNY: A Visual Interpretation Framework for Convolutional Neural Networks from a Necessary and Sufficient Perspective.")," Xiwei Xuan, Ziquan Deng, Hsuan-Tien Lin, Zhaodan Kong, Kwan-Liu Ma."),i.a.createElement("li",null,i.a.createElement("b",null,"P11"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P11.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits.")," Maximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek, Sebastian Lapuschkin."),i.a.createElement("li",null,i.a.createElement("b",null,"P12"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P12.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Semantic Approach to Quantifying the Consistency of Diffusion Model Image Generation.")," Brinnae Bent."),i.a.createElement("li",null,i.a.createElement("b",null,"P13"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P13.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Quantifying Explainability with Multi-Scale Gaussian Mixture Models.")," Anthony Rhodes, Yali Bian, Ilke Demir.")))),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Non-proceedings Track")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"P14"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P14.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Identifying Spurious Correlations using Counterfactual Alignment.")," Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari."),i.a.createElement("li",null,i.a.createElement("b",null,"P15"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P15.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models.")," Matthew Kowal, Richard P. Wildes, Konstantinos G. Derpanis."),i.a.createElement("li",null,i.a.createElement("b",null,"P16"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P16.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Utility-Fairness Trade-Offs and How to Find Them.")," "),i.a.createElement("li",null,i.a.createElement("b",null,"P17"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P17.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," FairerCLIP: Debiasing Zero-Shot Predictions of CLIP in RKHSs.")," "),i.a.createElement("li",null,i.a.createElement("b",null,"P18"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P18.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data.")," Yuhui Zhang*, Elaine Sui*, Serena Yeung-Levy."),i.a.createElement("li",null,i.a.createElement("b",null,"P19"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P19.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models.")," Nicholas Bai*, Rahul Ajay Iyer*, Tuomas Oikarinen, Tsui-Wei Weng."),i.a.createElement("li",null,i.a.createElement("b",null,"P20"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P20.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," An Image Patch Row-Column Ranking Method Using the Feature Accumulation Matrix to Explain Decisions of a Convolutional Neural Network.")," Luna M. Zhang."),i.a.createElement("li",null,i.a.createElement("b",null,"P21"),i.a.createElement(C.a,{target:"_blank",rel:"noopener",href:"./assets/papers2024/P21.pdf"}," ",i.a.createElement(K.a,{fontSize:"inherit"})),i.a.createElement("b",null," DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations.")," Maximilian Augustin, Yannic Neuhaus, Matthias Hein.")))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Organizers")),i.a.createElement(ie,null),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Program Committee")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"We thank our wonderful program committee members who made this workshop possible!"),i.a.createElement("br",null),i.a.createElement("br",null),"Aditya Chattopadhyay, Aditya Chinchure, Amin Parchami-Araghi, Angelos Nalmpantis, Anirban Sarkar, Anmol Kalia, Chenyang Zhao, Chirag Shukla, Deepti Ghadiyaram, Dhruv Srikanth, Eunji Kim, Gaurav Bhatt, Hubert Baniecki, Indu Panigrahi, Jawad Tayyub, John Gkountouras, Jonathan Donnelly, Joseph Paul Cohen, Julien Colin, Katelyn Morrison, Kwan Ho Ryan Chan, Lan Wang, Laura O'Mahony, Lenka T\u011btkov\xe1, Manxi Lin, Matthew Kowal, Muhammad Sarmad, Navdeeppal Singh, Nhi Pham, Nina Weng, Piotr Komorowski, Pushkar Shukla, Robin Hesse, Romain Xu-Darme, Sadaf Gulshad, Satyapriya Krishna, Sebastian Bordt, Simone Schaub-Meyer, Stefan Kolek, Sukrut Rao, Sunnie S. Y. Kim, Sweta Mahajan, Syed Nouman Hasany, Teresa Dorszewski, Tobias Labarta, Vikram V. Ramaswamy, Vipin Pillai."))),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"Contact")),i.a.createElement(V.a,{container:!0,justify:"flex-start"},i.a.createElement(V.a,{item:!0,xs:12,lg:9,className:e.gridItem},i.a.createElement(T.a,{className:e.SectionHeader,variant:"body2",align:"left"},"Email: ",i.a.createElement(C.a,{href:"mailto:xai4cv2024@googlegroups.com"},"xai4cv2024@googlegroups.com")))),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"h5",align:"left"},"(Closed) Call for Papers & Demos")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"We welcome paper and demo submissions.",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Papers")," should describe high-quality, original research. Contributions can include novel XAI methods; applications of existing methods on new domains, models, and tasks; evaluation or analysis of existing methods; and practical toolboxes."),i.a.createElement("li",null,i.a.createElement("b",null,"Demos")," should consist of static or interactive presentations of XAI for CV models and tasks, accompanied by a description. Contributions can include visualizations, explanations, and explorations of novel XAI systems; novel visualizations, explanations, and explorations of existing XAI systems; studies of how different  visualizations, explanations, and explorations of XAI systems are perceived by people; among others.")),"We have two tracks of submissions.",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Proceedings track:")," We welcome ",i.a.createElement("b",null,"max 4-page")," submissions of papers and demos. Submissions accepted to this track ",i.a.createElement("b",null,"will be published")," in the CVPR workshop proceedings."),i.a.createElement("li",null,i.a.createElement("b",null,"Non-proceedings track:")," We welcome ",i.a.createElement("b",null,"max 2-page"),' submissions (commonly referred to as "extended abstracts") of papers and demos. For the non-proceedings track, we encourage submissions of published or accepted work (e.g., papers and demos accepted to the CVPR main program). Submissions accepted to this track ',i.a.createElement("b",null,"will ",i.a.createElement("i",null,"not")," be published")," in the CVPR workshop proceedings.")))),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Timeline")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"Proceedings track")," ",i.a.createElement("br",null),i.a.createElement("s",null,"Submission deadline: ",i.a.createElement("b",null,"March 17, 2024 (5pm PT)"))," ",i.a.createElement("br",null),i.a.createElement("s",null,"Notification to authors (accept as spotlight, accept as poster, or reject): ",i.a.createElement("b",null,"April 6, 2024"))," ",i.a.createElement("br",null),i.a.createElement("s",null,"Camera-ready hard deadline: ",i.a.createElement("b",null,"April 14, 2024 (11:59pm PT)")),i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Non-proceedings track")," ",i.a.createElement("br",null),i.a.createElement("s",null,"Submissions deadline (to be considered for spotlights): ",i.a.createElement("b",null,"March 17, 2024 (5pm PT)"))," ",i.a.createElement("br",null),i.a.createElement("s",null,"Notification to authors (accept as spotlight, accept as poster, or reject): ",i.a.createElement("b",null,"April 6, 2024"))," ",i.a.createElement("br",null),i.a.createElement("s",null,"Rolling submissions and notifications (accept as poster or reject): ",i.a.createElement("b",null,"Until April 22, 2024"))," ",i.a.createElement("br",null),i.a.createElement("s",null,"Camera-ready deadline: ",i.a.createElement("b",null,"May 8, 2024 (5pm PT)")," ",i.a.createElement("b",null,"June 14, 2024 (5pm PT)")))),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Submission instructions")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},"All submissions should be in the ",i.a.createElement("b",null,"anonymized")," CVPR 2024 format available at ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines"},i.a.createElement("b",null,"https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines")),".",i.a.createElement("br",null),"The page limits ",i.a.createElement("b",null,"do not")," include references.",i.a.createElement("br",null),"You may optionally upload supplementary material. Reviewers will be encouraged to look at it, but are not obligated to do so.",i.a.createElement("br",null),"Submissions can be done at ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://cmt3.research.microsoft.com/XAI4CV2024"},i.a.createElement("b",null,"https://cmt3.research.microsoft.com/XAI4CV2024")),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Attendance & Presentation")),i.a.createElement("br",null),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("b",null,"Posters: ")," All accepted submissions will be invited to participate in an ",i.a.createElement("b",null,"in-person")," poster session at our workshop. Additionally, the authors will be asked to upload their posters which will be hosted on our webpage.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Spotlights: ")," We will pick 6-8 works among the submissions to be presented as spotlights. Presentations can either be ",i.a.createElement("b",null,"in-person")," or ",i.a.createElement("b",null,"pre-recorded"),".",i.a.createElement("br",null),i.a.createElement("br",null),"Abiding by the ",i.a.createElement("a",{target:"_blank",rel:"noopener",href:"https://cvpr.thecvf.com/Conferences/2024/Pricing2"},i.a.createElement("b",null,"CVPR guidelines")),", all accepted papers ",i.a.createElement("b",null,"must be presented by one of the authors"),".")),i.a.createElement(V.a,{item:!0,xs:12,className:e.gridItem},i.a.createElement(T.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},i.a.createElement("b",null,"Topics")),i.a.createElement(T.a,{className:e.sectionHeader,variant:"body2",align:"left"},i.a.createElement("ul",null,i.a.createElement("li",null,"Interpretable-by-design CV models"),i.a.createElement("li",null,"Post-hoc explanations of CV models"),i.a.createElement("li",null,"Evaluation and analysis of XAI for CV"),i.a.createElement("li",null,"Applications of XAI for CV"),i.a.createElement("li",null,"Methods for new interactions with CV models (e.g., debugging, editing)"),i.a.createElement("li",null,"Multimodal XAI, including both multimodal explanations of CV models and (unimodal) explanations of multimodal models"),i.a.createElement("li",null,"Datasets for developing and evaluating XAI for CV"),i.a.createElement("li",null,"Visualizations and toolboxes for XAI for CV"),i.a.createElement("li",null,"Human-centered XAI for CV (e.g., human evaluations, qualitative studies)"),i.a.createElement("li",null,"Studies of XAI for CV and related topics (e.g., fairness, transparency, interpretability, trust)")))),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container}),i.a.createElement("div",{className:e.container})))))))}}]),t}(i.a.Component),me=Object(M.a)(function(e){return{content:{margin:"0 auto",marginTop:"1.5em"},root:{},gridItem:{padding:e.spacing(1.5)},sectionHeader:{marginTop:"0.15em"},container:{padding:e.spacing(2)},footer:{marginTop:"0.15em",fontSize:14},people:{margin:"0 auto",marginTop:"1.5em"}}})(ce),ue=function(e){Object(u.a)(t,e);var a=Object(p.a)(t);function t(){return Object(c.a)(this,t),a.apply(this,arguments)}return Object(m.a)(t,[{key:"render",value:function(){return i.a.createElement(d.a,{theme:b},i.a.createElement(h.a,{basename:""},i.a.createElement("div",{className:"App"},i.a.createElement("header",{className:"App-header"},i.a.createElement(H,null)),i.a.createElement("div",null,i.a.createElement(E.b,{path:"/",exact:!0,component:j}),i.a.createElement(E.b,{path:"/workshop_cvpr22",exact:!0,component:J}),i.a.createElement(E.b,{path:"/workshop_cvpr22/"},i.a.createElement(E.a,{to:"/workshop_cvpr22"})),i.a.createElement(E.b,{path:"/workshop_cvpr23",exact:!0,component:le}),i.a.createElement(E.b,{path:"/workshop_cvpr23/"},i.a.createElement(E.a,{to:"/workshop_cvpr23"})),i.a.createElement(E.b,{path:"/workshop_cvpr24",exact:!0,component:me}),i.a.createElement(E.b,{path:"/workshop_cvpr24/"},i.a.createElement(E.a,{to:"/workshop_cvpr24"}))))))}}]),t}(i.a.Component);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));o.a.render(i.a.createElement(ue,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},75:function(e){e.exports={a:[{img_url:"assets/images/filip_radenovic.jpeg",name:"Filip Radenovic",organization:"Meta AI",website:"https://filipradenovic.github.io"},{img_url:"assets/images/abhishek_kadian.jpg",name:"Abhishek Kadian",organization:"Meta AI",website:"https://abhiskk.github.io"},{img_url:"assets/images/deepti_ghadiyaram.png",name:"Deepti Ghadiyaram",organization:"Meta AI",website:"https://deeptigp.github.io"},{img_url:"assets/images/dhruv_mahajan.jpeg",name:"Dhruv Mahajan",organization:"Meta AI",website:"https://ai.facebook.com/people/dhruv-mahajan"},{img_url:"assets/images/devi_parikh.jpeg",name:"Devi Parikh",organization:"Meta AI",website:"https://faculty.cc.gatech.edu/~parikh"},{img_url:"assets/images/vignesh_ramanathan.jpeg",name:"Vignesh Ramanathan",organization:"Meta AI",website:"https://ai.facebook.com/people/vignesh-ramanathan"},{img_url:"assets/images/jun_xie.jpeg",name:"Jun Xie",organization:"Meta AI",website:"https://www.linkedin.com/in/junxie19"}]}},76:function(e){e.exports={a:[{img_url:"assets/images/antonio_torralba.jpeg",name:"Antonio Torralba",organization:"MIT",website:"http://web.mit.edu/torralba/www"},{img_url:"assets/images/yixin_wang.jpeg",name:"Yixin Wang",organization:"University of Michigan",website:"https://yixinwang.github.io"},{img_url:"assets/images/rich_caruana.jpeg",name:"Rich Caruana",organization:"MSR",website:"https://www.microsoft.com/en-us/research/people/rcaruana"},{img_url:"assets/images/been_kim.jpeg",name:"Been Kim",organization:"Google Brain",website:"https://beenkim.github.io"},{img_url:"assets/images/trevor_darrell.jpeg",name:"Trevor Darrell",organization:"UC Berkeley",website:"https://people.eecs.berkeley.edu/~trevor"},{img_url:"assets/images/pradeep_ravikumar.jpeg",name:"Pradeep Ravikumar",organization:"Carnegie Mellon University",website:"https://www.cs.cmu.edu/~pradeepr"},{img_url:"assets/images/serena_yeung.jpeg",name:"Serena Yeung",organization:"Stanford",website:"https://ai.stanford.edu/~syyeung"},{img_url:"assets/images/hima_lakkaraju.jpeg",name:"Hima Lakkaraju",organization:"Harvard University",website:"https://himalakkaraju.github.io"}]}},77:function(e){e.exports={a:[{img_url:"assets/images/sunnie_kim.jpeg",name:"Sunnie S. Y. Kim",organization:"Princeton University",website:"https://sunniesuhyoung.github.io/"},{img_url:"assets/images/vikram_ramaswamy.jpeg",name:"Vikram V. Ramaswamy",organization:"Princeton University",website:"https://www.cs.princeton.edu/~vr23/"},{img_url:"assets/images/ruth_fong.png",name:"Ruth Fong",organization:"Princeton University",website:"https://www.ruthfong.com/"},{img_url:"assets/images/filip_radenovic.jpeg",name:"Filip Radenovic",organization:"Meta AI",website:"https://filipradenovic.github.io"},{img_url:"assets/images/abhi_dubey.jpeg",name:"Abhimanyu Dubey",organization:"Meta AI",website:"https://abhimanyudubey.github.io/"},{img_url:"assets/images/deepti_ghadiyaram.png",name:"Deepti Ghadiyaram",organization:"Runway",website:"https://deeptigp.github.io"}]}},78:function(e){e.exports={a:[{img_url:"assets/images/vera_liao.jpeg",name:"Q. Vera Liao",organization:"Microsoft Research Montr\xe9al",website:"http://qveraliao.com/"},{img_url:"assets/images/mohit_bansal.png",name:"Mohit Bansal",organization:"UNC Chapel Hill",website:"https://www.cs.unc.edu/~mbansal/"},{img_url:"assets/images/marina_hohne.jpeg",name:"Marina M.-C. H\xf6hne (n\xe9e Vidovic)",organization:"University of Potsdam & Leibnitz-Institution for Agriculture and Bioeconomy",website:"https://www.atb-potsdam.de/en/about-us/team/staff-members/person/marina-hohne"},{img_url:"assets/images/arvind_satyanarayan.jpeg",name:"Arvind Satyanarayan",organization:"MIT",website:"https://arvindsatya.com/"},{img_url:"assets/images/alice_xiang.jpeg",name:"Alice Xiang",organization:"Sony AI",website:"https://ai.sony/people/Alice-Xiang/"},{img_url:"assets/images/david_bau.png",name:"David Bau",organization:"Northeastern University",website:"https://baulab.info/"}]}},79:function(e){e.exports={a:[{img_url:"assets/images/indu_panigrahi.jpeg",name:"Indu Panigrahi",organization:"Princeton University",website:"https://xai4cv.github.io/"},{img_url:"assets/images/sunnie_kim.jpeg",name:"Sunnie S. Y. Kim",organization:"Princeton University",website:"https://sunniesuhyoung.github.io/"},{img_url:"assets/images/vikram_ramaswamy.jpeg",name:"Vikram V. Ramaswamy",organization:"Princeton University",website:"https://www.cs.princeton.edu/~vr23/"},{img_url:"assets/images/sukrut_rao.png",name:"Sukrut Rao",organization:"Max Planck Institute for Informatics",website:"https://sukrutrao.github.io/"},{img_url:"assets/images/stefan_kolek.jpg",name:"Stefan Kolek",organization:"LMU Munich",website:"https://skmda37.github.io/"},{img_url:"assets/images/lenka_tetkova.jpg",name:"Lenka T\u011btkov\xe1",organization:"Technical University of Denmark",website:"https://www.linkedin.com/in/lenka-tetkova"},{img_url:"assets/images/jawad_tayyub.jpg",name:"Jawad Tayyub",organization:"Endress + Hauser",website:"https://xai4cv.github.io/"},{img_url:"assets/images/katelyn_morrison.png",name:"Katelyn Morrison",organization:"Carnegie Mellon University",website:"https://katelyn98.github.io/"},{img_url:"assets/images/pushkar_shukla.jpeg",name:"Pushkar Shukla",organization:"Toyota Technolgical Institute at Chicago",website:"https://pushkershukla.github.io/shukla.github.io/"},{img_url:"assets/images/deepti_ghadiyaram.png",name:"Deepti Ghadiyaram",organization:"Runway",website:"https://deeptigp.github.io"}]}},80:function(e){e.exports={a:[{img_url:"assets/images/tim_miller.png",name:"Tim Miller",organization:"The University of Queensland",website:"https://eecs.uq.edu.au/profile/9477/tim-miller"},{img_url:"assets/images/suin_lee.jpeg",name:"Su-In Lee",organization:"University of Washington",website:"https://aims.cs.washington.edu/su-in-lee"},{img_url:"assets/images/kristian_kersting.jpeg",name:"Kristian Kersting",organization:"TU Darmstadt",website:"https://ml-research.github.io/people/kkersting/"},{img_url:"assets/images/bernt_schiele.jpeg",name:"Bernt Schiele",organization:"MPI for Informatics",website:"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele"},{img_url:"assets/images/leonid_sigal.jpeg",name:"Leonid Sigal",organization:"University of British Columbia",website:"https://www.cs.ubc.ca/~lsigal/"},{img_url:"assets/images/vineeth_balasubramanian.jpg",name:"Vineeth N Balasubramanian",organization:"Indian Institute of Technology, Hyderabad",website:"https://people.iith.ac.in/vineethnb/"}]}},88:function(e,a,t){e.exports=t(104)},93:function(e,a,t){},94:function(e,a,t){},95:function(e,a,t){}},[[88,1,2]]]);
//# sourceMappingURL=main.398149f0.chunk.js.map